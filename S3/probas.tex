\documentclass[10pt,a4paper,french]{article}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{../latex/clovisai}
\usepackage{array}

\begin{document}

\title{Probabilités \& Statistiques}
\author{Ivan Canet}
\maketitle

\tableofcontents

\part{Probabilités}

\section{Arrangements}

On distingue deux manières de dénombrer ;

\begin{itemize}
\item Lorsque l'ordre est important, on dit qu'il s'agit d'un \textbf{arrangement},
\item Lorsque l'ordre n'est pas important, on dit qu'il s'agit d'une \textbf{combinaison}.
\end{itemize}

\subsection{Arrangement}

\paragraph{Exemple}
On décide de choisir 5 chevaux parmi 5. Le nombre de possibilités est $20 \times 19 \times 18 \times 17 \times 16$, ce qui est égal à $\frac{20!}{15!}$, dont on peut déduire $\frac{20!}{(20-5)!}$.

\paragraph{Formule \& notation}
On note ``$p$ éléments parmi $n$'' $\arrang{p}{n}$, et la valeur est \[ \arrang{p}{n} = \frac{n!}{(n-p)!} \]

\subsection{Combinaison}

\paragraph{Exemple}
En reprenant l'exemple précédent des 5 chevaux parmi 20; si l'ordre n'est pas important, alors on peut regrouper toutes les possibilités où les mêmes chevaux ont été tirés dans un ordre différent. Il y a 5 chevaux, donc il y a $5!$ cas où les mêmes chevaux ont été tirés. Il faut donc enlever ces cas, ce qui donne un nombre de possibilités de $\frac{A_n^p}{p!}$.

\paragraph{Formule \& notation}
On note ``$p$ éléments parmi $n$'' $\parmi{p}{n}$, de formule \[ \parmi{p}{n} = \frac{n!}{p! (n-p)!} \].

\subsection{Triangle de Pascal}

Le triangle de Pascal (voir \autoref{tab:pascal}) se construit grâce à un tableau dans lequel chaque case vaut la valeur de celle du dessus plus celle en diagonale au dessus à gauche.

\begin{table}
\caption{\label{tab:pascal} Triangle de Pascal}
\centering
\begin{tabular}{l|cccccccccc}
n $\backslash$ p & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline
1  & 1 & 1 &   &   &   &   &   &   &   &   \\
2  & 1 & 2 & 1 &   &   &   &   &   &   &   \\
3  & 1 & 3 & 3 & 1 &   &   &   &   &   &   \\
4  & 1 & 4 & 6 & 4 & 1 &   &   &   &   &   \\
5  & 1 & 5 & 10 & 10 & 5 & 1 &   &   &   &   \\
6  & 1 & 6 & 15 & \textbf{20} & 15 & 6 & 1 &   &   &   \\
7  & 1 & 7 & 21 & 35 & 35 & 21 & 7 & 1 &   &   \\
8  & 1 & 8 & 28 & 56 & 70 & 56 & 28 & 8 & 1 &   \\
9 & 1 & 9 & 36 & 84 & 126 & 126 & 84 & 36 & 9 & 1 \\
\end{tabular}
\end{table}

On peut l'utiliser pour retrouver que $\parmi{4}{6}=20$.

On retrouve aussi que \[ \parmi{p}{n} = \parmi{p}{n-1} + \parmi{p-1}{n-1} \].

\subsection{Applications}

\paragraph{Application}
Comme une fonction, mais tous les éléments de l'ensemble de départ doivent absolument avoir une image (on ne peut pas avoir de valeurs interdites).

\paragraph{Application surjective}
Tout élément de l'ensemble d'arrivée a au moins un antécédent.

\section{Définitions}

\paragraph{Un événement élémentaire} est un élément qui ne peut pas être séparé entre d'autres éléments (eg. dans un lancer de dés, ``tirer 1'' est un événement élémentaire).

\paragraph{Un événement non-élémentaire} peut être défini comme un ensemble d'événements élémentaires (eg. dans un lancer de dés, ``tirer un nombre pair'' est un événement non-élémentaire).

\paragraph{L'univers} est l'ensemble de tous les événements élémentaires, on le note $\Omega$.

\paragraph{Le cardinal} est le nombre d'événements élémentaires dans un événement non-élémentaire (la taille), on le note $card(X)$ où $X$ est l'ensemble.

\paragraph{Calcul de probabilité} La probabilité d'un événement $E$ est notée $P(E)$ et vaut \[ P(E) = \frac{card(E)}{card(\Omega} \]

\paragraph{Probabilité conditionnelle}
``La probabilité de A quand B est vérifiée'' s'écrit: \[ P_B(A)=\frac{P(A \cap B)}{P(B)} \]

\section{Variables}

\subsection{Variables aléatoires}

L'espérance est la moyenne des probabilités : \[ E(X) = \sum x_i P(X=x_i) \]

Pour deux variables aléatoires $X$ et $Y$, $E(X+Y) = E(X) + E(Y)$.

Par contre, $E(X \times Y) = E(X) \times E(Y)$ uniquement si les deux événements sont indépendants.

\paragraph{Exemple}

On jette un dé non truqué, on note la variable $X$ qui vaut le double de sa valeur. L'univers est $\Omega=\lbrace 2, 4, 6, 8, 10, 12 \rbrace$. Voir le \autoref{tab:var-alea-1}.

\begin{table}[h]
\caption{\label{tab:var-alea-1} Exemple de variable aléatoire: double du nombre tiré par un dé}
\centering
\begin{tabular}{c|cccccc}
$X=x_i$ & 2 & 4 & 6 & 8 & 10 & 12\\
\hline
$P(X=x_i)$ & $1/6$ & $1/6$ & $1/6$ & $1/6$ & $1/6$ & $1/6$\\
\end{tabular}
\end{table}

On pose une variable $Y$ qui vaut 1 si le résultat est pair et 3 si le résultat est impair. Puisque les chances sont équiprobables, $P(Y=1) = \frac{1}{2}$ et $P(Y=3) = \frac{1}{2}$.

On s'intéresse à $X+Y$ et $X \times Y$. Le cas où $Y=3$ et $X=2$ est impossible, parce que cela veut dire que le nombre d'origine du lancer de dé était 1 et pair. On mets 0 à chacune de ces possibilités; voir \autoref{tab:var-alea-2} et \autoref{tab:var-alea-3}.

\begin{table}[h]
\caption{\label{tab:var-alea-2} Somme de deux variables aléatoires}
\centering
\begin{tabular}{c|cc}
$X \backslash Y$ & 1 & 3\\
\hline
2 & 3 & 0\\
4 & 0 & 7\\
6 & 7 & 0\\
8 & 0 & 11\\
10 & 11 & 0\\
12 & 0 & 15\\
\end{tabular}
\end{table}

\begin{table}[h]
\caption{\label{tab:var-alea-3} Probabilités de $P(X+Y = x_i)$}
\centering
\begin{tabular}{c|cccc}
$x_i$ & 3 & 7 & 11 & 15\\
\hline
$P(X+Y=x_i)$ & 1/6 & 2/6 & 2/6 & 1/6\\
\end{tabular}
\end{table}

\subsection{Variables discrètes}

On note $E(X)$ l'espérance (la moyenne).
\[
E(X) = \sum_{i=1}^n x_i P(x_i)
\]

La fonction de répartition, $F(k)$, est la probabilité que plusieurs événements se réalisent.
\[
F(k) = P(X \leq k) = \sum_{i=1}^k P(x_i)
\]

La variance est notée $V(X)$ ;
\[
V(X) = E[(X-E(X))^2] = E(X^2) - E(X)^2
\]

L'écart-type est noté $\sigma(X)$ ;
\[
\sigma(X) = \sqrt{V(X)}
\]

\subsection{Variables continues}

L'espérance $E(X)$ est:
\[
E(X)= \int_a^x f(t) \text{d} t
\]

La fonction de répartition $F(u)$ est:
\[
F(u) = \int_a^u f(t) \text{d} t
\]

\section{Lois discrètes}

\subsection{Loi binomiale}

Dans le cas d'une suite d'expériences élémentaires indépendantes à deux issues possibles, on utilise la loi binomiale.

On note $p$ la probabilité d'une réussite et $n$ le nombre d'expériences (par exemple pour 12 lancers de pièce: $p=1/2$ et $n=12$). On note cette loi $B(n; p)$, dite ``de Bernouilli''. On peut alors créer la variable aléatoire suivant cette loi : \[ X \suit B(n; p) \]

On peut calculer la probabilité qu'il y ai $k$ succès (où $k \in [0; n]$):
\[
P(X=k) = \parmi{k}{n} p^k (1-p)^{n-k}
\]

\paragraph{Exemple}
Pour un lancer de 4 fléchettes successivement avec une probabilité de réussite de $0.6$, on a $p=0.6$ et $n=4$ donc la probabilité de toucher exactement 3 fois est \[ P(X=3) = \parmi{3}{4} \times 0.6^3 \times 0.4^1 \].

L'espérance $E(X)$ s'écrit: \[E(X)=n \times p\]

L'écart-type $\sigma(X)$ s'écrit: \[\sigma(X)=\sqrt{np(1-p)}\]

La variance $V(X)$ s'écrit: \[V(X) = \sigma(X)^2\]

\subsection{Loi poisson}
Dans le cas où il s'agit d'un événement rare (la probabilité $p$ est faible et le nombre d'essais $n$ est élevé), on peut approximer la loi binomiale par la loi poisson pour obtenir la probabilité de réussite dans un intervalle de temps (eg. apparition de pannes dans un réseau informatique). La loi de poisson correspond à la loi binomiale $B(n, \frac{\lambda}{n})$. On peut donc trouver :
\[
P(X = k) \approx \lim_{n \to \infinity} P(X_n = k) = \exp(-\lambda) \frac{\lambda^k}{k!}
\] où $\lambda$ est le nombre de fois que l'événement se produit en moyenne ($E(X) = \lambda$).

On peut chercher soit ``le nombre de réalisations dans un intervalle donné'' ou ``le temps entre deux réalisations d'un événement''.

Deux événements successifs doivent être indépendants.

L'événement doit être suffisamment rare pour ne pas se réaliser plusieurs fois en un petit intervalle de temps.

Quand il y a plus de 20 tentatives ($n$), avec une probabilité plus petite que 0.1 ($p$), et il faut que la moyenne ($np$) soit plus petit que 5. Dans ce cas : \[B(n; p) \approx P(np)\]

\subsection{Loi géométrique}

La loi géométrique est une variante de la loi binomiale : au lieu de chercher le ``nombre de réussites en $n$ essais'', on cherche ``combien d'essais sont nécessaires pour obtenir une réussite''.
\[
P(X = n) = q^{n-1} p
\] où $p$ est la probabilité, $q$ la probabilité de l'échec (donc $q = 1-p$) et $P(X = n)$ est la probabilité de réussir au $n$\ieme~tirage.

On peut couper la formule entre $q^{n-1}$ qui est la probabilité d'avoir $n-1$ échecs à la suite, puis $p$ la probabilité d'avoir un succès (``avoir un succès au bout du $n$\ieme~tirage'' = ``avoir $n-1$ échecs'' puis ``réussir une fois'').

On trouve l'espérance: \[E(X) = \frac{1}{q}\]

La variance: \[V(X) = \frac{q}{p^2}\]

L'écart-type: \[\sigma(X) = \frac{\sqrt{q}}{p}\]

La loi géométrique est une loi sans mémoire : on peut ignorer le passé : donc on peut ignorer les probabilités conditionnelles.

\section{Lois continues}

\subsection{Loi normale}

La loi normale a deux paramètres: \[N(E, \sigma)\].

On peut donc approximer une loi binomiale par une loi normale: \[ B(n, p) \approx N(np, \sqrt{npq}) \] dès que $n \geq 30$, $np \geq 15$ et $nq \geq 15$.

On peut aussi approximer une loi poisson par une loi normale: \[P(\lambda) \approx N(\lambda, \sqrt{\lambda})\] dès que $\lambda \geq 16$.

Cela permet de trouver des cas comme $P(X \leq k)$ où $k$ est un entier.

\paragraph{Remarque}
Pour $k$ un entier, $P(X = k) = 0$.

\paragraph{Correction de continuité}
Puisque l'on passe d'une loi discrète à une loi continue, il faut transformer les valeurs: \[ P(X > n) = P(X \geq n+1) \approx P(X \geq n+0.5) \]
par exemple, $P(X > 300)$ devient $P(X \geq 300.5)$.

\paragraph{Calcul de probabilités grâce à une table}

La table permet de trouver la probabilité de $P(X < n)$ où $n \geq 0$ ; pour $X \suit N(0, 1)$.

Si on veut calculer d'autres probabilités, il faut les remplacer:
\begin{itemize}
\item $P(X > n) = 1 - P(X < n)$
\item $P(X < -n) = 1 - P(X < n)$
\item $P(X > -n) = P(X < n)$
\item $P(a < X < b) = P(X < b) - P(X < a)$ où $(a,b) \in \setR^2$
\item $P(\vert X \vert > n) = P(Z > n \text{ ou } Z < -n) = 2 P(Z > n) = 2 (1-P(Z < n))$
\end{itemize}

\paragraph{Centrer une loi}
Pour une variable aléatoire $X$ suivant une loi normale de paramètres $\mu, \sigma$; on peut la centrer en: 
\[Y = \frac{X - \mu}{\sigma} \suit N(0, 1)\]

\subsection{Loi exponentielle}

La loi exponentielle est définie par:
\begin{itemize}
\item Si $x < 0$ : 0
\item Si $x \geq 0$ : $\lambda e^{-\lambda x}$
\end{itemize}

On l'utilise très souvent pour des durées de vie.

L'espérance est \[E(X) = \frac{1}{\lambda}\]

L'écart-type est \[ \sigma(X) = \frac{1}{\lambda} \]

Pour calculer $P(a < X < b)$ : \[ \int_a^b \lambda e^{-\lambda t} dt = \left[ -e^{-\lambda t} \right]_a^b = -e^{-\lambda b} + e^{-\lambda a} \]

Par exemple pour $P(50 < X < 100)$ : \[ \int_{50}^{100} \lambda e^{-\lambda t} dt = \left[ -e^{-\lambda t} \right]_{50}^{100} = -e^{-\lambda 100} + e^{-\lambda 50} \]

Si on cherche $P(X \geq n)$, on calcule l'inverse: \[ P(X \geq n) = 1 - P(X \leq n) \] ce qui est beaucoup plus simple à calculer.

\paragraph{Remarque}
La loi exponentielle est une loi sans vieillissement: \[ P_B(a < X < b) = P(a < X < b) \]

\section{Comment choisir la loi}

La loi d'origine est soit:

\begin{itemize}
\item Une \textbf{loi binomiale} dans la plupart des cas, de paramètres $p$ la probabilité et $n$ le nombre d'essais,
\item Une \textbf{loi poisson} dans les cas où les événements sont rares, de paramètre $\lambda$ la moyenne de réussites dans une période donnée, quand on cherche le temps nécessaire,
\item Une \textbf{loi géométrique} si on cherche le nombre d'essais nécessaires pour le premier échec (ou la première réussite),
\item S'il s'agit d'événements dépendants, on peut aussi utiliser les \textbf{probabilités conditionnelles}.
\item Si on dépend du temps, qu'il y a diminution, et qu'on a une moyenne de décroissance, on utilise la \textbf{loi exponentielle}.
\item On peut convertir les trois premières lois en \textbf{loi normale} pour obtenir des probabilités ``inférieure'' de la forme $P(X \leq k)$.
\end{itemize}

\part{Statistiques}

\section{Analyse unidimensionnelle}

On s'intéresse à une seule variable aléatoire, souvent notée $X$.

\subsection{Caractère discret}

Pour une population de $n$, on a plusieurs valeurs appelées $v_0, v_1 \ldots v_n$.

Plusieurs individus peuvent avoir une même valeur (par exemple, plus élèves peuvent avoir la même note) : on note $x$ chaque valeur possibles; donc $x_0, x_1 \ldots x_k$. À noter que $k \leq n$ puisqu'on ne peut pas avoir plus de résultats que d'individus !

\paragraph{Effectif}
L'effectif de la variable $x_i$ correspond au nombre d'individus qui ont eu ce résultat (par exemple, le nombre d'étudiants ayant eu 15): on le note $n_i$.

\paragraph{Fréquence}
La fréquence de la variable $x_i$ est la ``probabilité'' que cela arrive; donc \[ f_i = \frac{n_i}{n} \]

\paragraph{Fréquence cumulée}
La fréquence cumulée est la somme de toutes les fréquences inférieures: \[ F_i = \sum_{j=1}^i f_j \]

\subsection{Paramètres de position}

\paragraph{Mode}
Pour trouver le mode, on calcule chaque fréquence. La fréquence la plus élevée permet de trouver le mode. Par exemple, si on a 70\% des étudiants qui ont eu 12, 20\% qui ont eu 13 et 10\% qui ont eu 14, le mode est 12.

\paragraph{Moyenne}
Notée $\overline{X}$ \[ \overline{X} = \frac{1}{n} \sum_{i=1}^k n_i x_i \]

\paragraph{Médiane}
La valeur telle que la moitié des valeurs est inférieure, et l'autre moitié est supérieure ; notée $M$.

\paragraph{1\ier~quartile}
La valeur telle que 25\% des valeurs sont inférieures, et 75\% des valeurs sont supérieures ; notée $Q_1$.

\paragraph{3\ieme~quartile}
La valeur telle que 75\% des valeurs sont inférieures, et 25\% des valeurs sont supérieures ; notée $Q_3$.

\subsection{Paramètres de dispersion}

\paragraph{Écart absolu moyen}
%TODO c'est un peu l'écart-type ?

\paragraph{Intervalle de variation}
L'espace entre la valeur maximale et la valeur minimale: \[ v_max - v_min \]

\paragraph{Intervalle inter-quartile}
L'espace entre les deux quartiles: \[ Q_3 - Q_1 \]

\paragraph{Variance}
%TODO

\section{Analyse bidimensionnelle}

L'analyse dimensionnelle permet d'étudier des corrélations.

\paragraph{Covariance}
\[ cov(X, Y) = \sigma_{X Y} = \frac{\sum_{i=1}^n (x_i - \overline{X}) (y_i - \overline{Y})}{n} \]
\[ \sigma_{X Y} = \overline{X Y} - \overline{X} \times \overline{Y} \]

\paragraph{Coefficient de corrélation}
\[ \rho_{X Y} = \frac{\sigma_{X Y}}{\sigma_X \sigma_Y} \]
avec
\[ -1 \leq \rho_{X Y} \leq 1 \]
on a aussi \[ \rho_{X Y} = \frac{<X', Y'>}{\Vert X \Vert \Vert Y \Vert} = cos(X', Y') \]

Si $\vert \rho_{X Y} \vert \approx 1$ veut dire que les deux séries sont corrélées.










\part{Exercices}

\paragraph{Exercice 10}
Désignons par $f_n$ le nombre de manières de jeter une pièce $n$ fois sans que deux piles successifs n'apparaissent.

Soit une partie en $n$ coups au cours de laquelle on n'obtient jamais deux piles consécutives :
\begin{itemize}
\item La partie commence par P, le second lancer doit donc donner F, ensuite toute partie en $n-2$ coups ne donnant pas deux piles consécutives convient; il y en a $f_{n-2}$.
\item La partie commence par F, le second lancer peut être quelconque donc F est suivi d'une quelconque partie en $n-1$ coups ne présentant pas deux piles consécutives. Il y en a $f_{n-1}$.
\end{itemize}

Donc, $f_n = f_{n-1} + f_{n-2}$ avec $f_0 = 1$ et $f_1=2$.

La probabilité d'un événement\footnote{On appelle Cardinal d'un événement (ou plus généralement d'un ensemble) le nombre d'éléments de cet ensemble. Un cardinal peut être fini ou infini.} est $P(A)=\frac{card(A)}{card(\Omega)}$, donc pour l'événement ``Nombre de manières de jeter une pièce $n$ fois sans 2 piles successifs'', l'univers est ``Nombre de manières de jeter une pièce $n$ fois'', ce qui donne $2^n$ possibilités.

On a donc $P(A)=\frac{f_n}{2^n}=\frac{f_{n-1}+f_{n-2}}{2^n}$.

\paragraph{Exercice 1}
Avec remise : $\Omega=\lbrace(R, V), (V, R), (V, B), (B, V), (B, R), (R, B), (R, R), (B, B), (V, V)\rbrace$ donc $card(\Omega)=3^2$ (tirage successif avec remise).

Sans remise : $\Omega=\lbrace(R, V), (V, R), (V, B), (B, V), (B, R), (R, B)\rbrace$ donc $card(\Omega)=3^2-3=3 \times 2$ (tirage successif sans remise).

\paragraph{Exercice 2} 50\% pratique la natation, 55\% pratique le basket, 22.5\% pratique les deux sports.

On en déduit que la probabilité qui pratique la natation et non le basket est $P(N \setminus B) = P(N) - P(N \cup B) = 50 \% - 22.5\% = 27.5 \% $.

De la même manière, la population qui pratique le basket mais pas la natation est $P(B \setminus N)= 55\% - 22.5\% = 32.5\%$.

La population que ne pratique aucun sport est $17.5\%$.

\paragraph{Exercice 3}
5 boules rouges, 6 bleues, 8 vertes ; on tire 3 boules au hasard.

L'univers a une taille de $card(\Omega) = {19 \choose 3}$.

Les événements sont $A_R$ ``toutes les boules sont rouges'', $A_B$ ``toutes les boules sont bleues'' et $A_V$ ``toutes les boules sont vertes''.
Ces trois événements forment une partition de $A$ ``toutes les boules sont de la même couleur'' ; donc 
$P(A) = P(A_R) + P(A_B) + P(A_V)$
avec
$card(A_R)={5 \choose 3}$,
$card(A_B)={8 \choose 3}$ et
$card(A_V)={8 \choose 3}$ ; donc
$P(A)=
\frac
	{{5 \choose 3} + {8 \choose 3} + {8 \choose 3}}
	{{19 \choose 3}}
$.

\paragraph{Exercice 4}
Il y a 3 manières de faire 4 (2+2, 3+1, 1+3) pour un univers d'une taille $6\times6$; donc $P(S=4)=\frac{3}{36}=\frac{1}{12}$.

Pour 10 lancers, il faut que au moins un ai une somme qui vaille 4. On reconnaît une loi binomiale avec $n=10$, $p=\frac{1}{12}$ et $k=$ ``plus de 1'' ; $X$ est le ``nombre de succès''.

$P(X \geq 1) = 1 - P(X=0) = 1 - {0 \choose 10} (\frac{1}{12})^0 (\frac{11}{12})^{10} = 1 - (\frac{11}{12})^{10}$.

\paragraph{Exercice 5}
Il y a $32 \choose 5$ distributions possibles.

\subparagraph{5 cartes de la même couleur}
Le nombre de distributions de 5 cartes de la même couleur est $card(A)={4 \choose 1}\times{8 \choose 5}$. La probabilité de $A$ est donc $P(A)=\frac{4 {8 \choose 5}}{{32 \choose 5}} = \frac{224}{201376}=0.11\%$.

\subparagraph{La dame et le roi de cœur}
La probabilité d'obtenir la dame et le roi de cœur parmi les 5 cartes tirées : $card(A)={1\choose 1}{1\choose 1}{30 \choose 3}$ donc $P(A)=\frac{{30 \choose 3}{1\choose 1}{1\choose 1}}{{32 \choose 5}}$.

\subparagraph{Exactement 3 valets}
La probabilité d'obtenir 3 valets parmi les 5 cartes : $card(A)={4 \choose 3}{28 \choose 2}$. On prend les deux dernières parmi 28 au lieu de parmi 29, parce que la 29\ieme~est le quatrième valet, qu'il ne faut pas prendre.

\subparagraph{Brelan: trois cartes de la même valeur}
La probabilité d'obtenir trois cartes de la même valeur : $card(A)={8 \choose 1} {4 \choose 3} {28 \choose 2}$ parce que l'on choisit la valeur, ensuite il faut en prendre trois de cette valeur (donc parmi 4) puis on prend aléatoirement les 2 dernières parmi ce qu'il reste dans le paquet.

\subparagraph{Carré: quatre cartes de la même valeur}
La probabilité d'obtenir 4 cartes de la même valeur (un carré) : $card(A)={8 \choose 1}{4 \choose 4}{28 \choose 1}$.

\subparagraph{Au moins un brelan}
La probabilité d'obtenir au moins un brelan : $card(A)={8 \choose 1}{4 \choose 3}{29 \choose 2}$.

\subparagraph{Quinte flush}
La probabilité d'obtenir une quinte flush : il y a 4 couleurs, pour chacune on peut faire 4 quintes flush, donc $P(A)=16$.

\subparagraph{Deux paires distinctes}
La probabilité d'obtenir deux paires distinctes : ${8 \choose 1}{4 \choose 2}{7 \choose 1}{4 \choose 2}{24 \choose 1}$ ; on prend une valeur, puis 2 cartes dans cette valeur; on prend une nouvelle valeur (différente donc parmi 7), puis à nouveau 2 cartes dans cette valeur ; on termine avec une carte parmi le reste du paquet.

\paragraph{Exercice 6}
On pose $A$ ``Au moins un 6'' et $B$ ``Les deux sont différents''.

Les cardinaux sont $card(A)=11$ (il n'y a pas d'ordre donc on ne compte pas $(6; 6)$) donc $P(A)=\frac{11}{36}$ et $card(B)=30$ donc $P(B)=\frac{30}{36}$.

On a aussi $card(A \cap B)=10$ donc $P(A \cap B)=\frac{10}{36}$.

Donc $P_B(A) = \frac{P(A \cap B)}{P(B)} = \frac{10 / 36}{30 / 36} = \frac{1}{3}$.

\paragraph{Exercice 7}
On a deux événements, B ``Les deux dernières sont des piques'' et A ``La première carte tirée est une pique''. Puisqu'un pioche 3 cartes, $card(\Omega) = \arrang{3}{52} = \frac{52!}{49!}$.

On cherche la probabilité $P_B(A)$. On sait qu'elle vaut $P_B(A) = \frac{P(A \inter B)}{P(B)}$ ; on sait que $A \inter B$ est ``Les trois cartes tirées sont des piques'', ce qui est simple à calculer, $card(A\inter B) = \arrang{3}{13}$ donc $P(A\inter B) = \frac{card(A\inter B)}{card(\Omega)} = \frac{13! \times 49!}{10! \times 52!}$.

On cherche ensuite $P(B)$ : soit la première carte est un pique ($13 \times 12 \times 11$), soit elle n'est pas un pique ($39 \times 13 \times 12$) ; donc

\[
P(B) = \frac{13 \times 12 \times 11 + 39 \times 13 \times 12}{52 \times 51 \times 50} = \frac{11}{50}
\]

$A$ et $B$ sont-ils indépendants ? $P_B(A)=\frac{11}{50}$ et $P(A)=\frac{1}{4}$ donc les deux événements ne sont pas indépendants.

\paragraph{Exercice 8}
On commence par dessiner l'arbre. On ajoute toutes les possibilités arrivant à $D$ pour trouver la somme $P(D)=0.027$, idem pour $P_D(A)=0.37$.

\paragraph{Exercice 3}
Soit $X$ la variable aléatoire comptant le nombre de personnes sur lesquelles le médicament n'a aucun effet. $X$ prend les valeurs $[0, 5]$. On a donc $X \suit B(5; 0.95)$. On cherche $P(X \geq 4) = P(X = 4) + P(X = 5) = \parmi{4}{5} 0.95^4 0.05^1 + \parmi{5}{5} 0.95^5 0.05^0$.

\paragraph{Exercice 4}
On a $n = 10$ et $p = \frac{9}{10}$. On pose une variable aléatoire $X \suit B(10; 0.9)$.

\paragraph{Exercice 5}
Sur 10 pièces fabriquées, une est défectueuse.

La probabilité qu'il y ai au plus 5 pièces défectueuses: $X \suit B(50; 0.1)$ et on cherche $P(X \leq 5) = P(X=0) + P(X=1) + \cdots + P(X=5)$. Ici on ne peut pas l'approximer par une loi normale, parce que les cas ne sont pas vérifiés.

On cherche $P(X = k) \geq 0.95$ avec $k = 50$ et $p=9/10$. On écrit donc $\parmi{50}{n} (\frac{3}{10})^{50} (\frac{1}{10})^{1-50} \geq 0.95$.

\paragraph{Exercice 6}
Une machine est composée de 50 éléments, qui ont une probabilité de $0.01$ de se casser. Si 4 se cassent, la machine est cassée.

On pose la loi binomiale $B(50, 0.01)$. On cherche $P(X < 4)=P(X=0) + P(X=1) + \cdots + P(X=3)$.

Puisque $n$ est grand et $p$ est petit, on peut utiliser une loi poisson: $P(0.5) \approx B(50, 0.01)$. En posant $Y \suit P(0.5)$, on a $P(Y < 4) = e^{0.5} \times \frac{0.5^4}{4!}$.

\paragraph{Exercice 7}
On a $N$ appels par heure. On suppose une loi de Poisson.

On prend l'heure comme unité de temps. Pour une heure, on a $\lambda = N$, donc $X \suit P(N)$. On cherche donc 3 appels par minute, donc 180 appels en une heure. $P(X=180)=e^{-180} \times \frac{N^{180}}{180!}$.

On approxime en loi normale: $Y \suit N(N, \sqrt{N})$ donc $P(Y \geq 3)$ est corrigé en $P(Y > 2.5)$. On utilise ensuite une table pour trouver le résultat.

\paragraph{Exercice 8}
On considère un hamster dans 5 compartiments.

Soit $X$ la variable aléatoire qui correspond au numéro de l'essai où le hamster sort. $X$ suit une loi géométrique de paramètres $p = \frac{1}{5}$. Donc, $P(X=1) = 1/5$, $P(X=3) = (\frac{4}{5})^2 \frac{1}{5}$ et $P(X=7) = (\frac{4}{5})^6 \frac{1}{5}$.

\paragraph{Exercice 3}
Ceci est du texte
\begin{table}[h]
\centering
\begin{tabular}{ccccc}
$x_i$ & $y_i$ & $x_i^2$ & $y_i^2$ & $x_i y_i$ \\
\hline
\ldots & \ldots & \ldots & \ldots & \ldots \\
$\overline{X}$ & $\overline{Y}$ & $\overline{X^2}$ & $\overline{Y^2}$ & $\overline{X Y}$
\end{tabular}
\end{table}
d'où on déduit $V(X)$, $V(Y)$, $\sigma(X)$ et $\sigma(Y)$.

\end{document}