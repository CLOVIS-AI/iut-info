\documentclass[10pt,a4paper,french]{article}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{../latex/clovisai}
\usepackage{array}

\begin{document}

\title{Probabilités \& Statistiques}
\author{Ivan Canet}
\maketitle

\tableofcontents

\part{Probabilités}

\section{Arrangements}

On distingue deux manières de dénombrer ;

\begin{itemize}
\item Lorsque l'ordre est important, on dit qu'il s'agit d'un \textbf{arrangement},
\item Lorsque l'ordre n'est pas important, on dit qu'il s'agit d'une \textbf{combinaison}.
\end{itemize}

\subsection{Arrangement}

\paragraph{Exemple}
On décide de choisir 5 chevaux parmi 5. Le nombre de possibilités est $20 \times 19 \times 18 \times 17 \times 16$, ce qui est égal à $\frac{20!}{15!}$, dont on peut déduire $\frac{20!}{(20-5)!}$.

\paragraph{Formule \& notation}
On note ``$p$ éléments parmi $n$'' $\arrang{p}{n}$, et la valeur est \[ \arrang{p}{n} = \frac{n!}{(n-p)!} \]

\subsection{Combinaison}

\paragraph{Exemple}
En reprenant l'exemple précédent des 5 chevaux parmi 20; si l'ordre n'est pas important, alors on peut regrouper toutes les possibilités où les mêmes chevaux ont été tirés dans un ordre différent. Il y a 5 chevaux, donc il y a $5!$ cas où les mêmes chevaux ont été tirés. Il faut donc enlever ces cas, ce qui donne un nombre de possibilités de $\frac{A_n^p}{p!}$.

\paragraph{Formule \& notation}
On note ``$p$ éléments parmi $n$'' $\parmi{p}{n}$, de formule \[ \parmi{p}{n} = \frac{n!}{p! (n-p)!} \].

\subsection{Triangle de Pascal}

Le triangle de Pascal (voir \autoref{tab:pascal}) se construit grâce à un tableau dans lequel chaque case vaut la valeur de celle du dessus plus celle en diagonale au dessus à gauche.

\begin{table}
\caption{\label{tab:pascal} Triangle de Pascal}
\centering
\begin{tabular}{l|cccccccccc}
n $\backslash$ p & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline
1  & 1 & 1 &   &   &   &   &   &   &   &   \\
2  & 1 & 2 & 1 &   &   &   &   &   &   &   \\
3  & 1 & 3 & 3 & 1 &   &   &   &   &   &   \\
4  & 1 & 4 & 6 & 4 & 1 &   &   &   &   &   \\
5  & 1 & 5 & 10 & 10 & 5 & 1 &   &   &   &   \\
6  & 1 & 6 & 15 & \textbf{20} & 15 & 6 & 1 &   &   &   \\
7  & 1 & 7 & 21 & 35 & 35 & 21 & 7 & 1 &   &   \\
8  & 1 & 8 & 28 & 56 & 70 & 56 & 28 & 8 & 1 &   \\
9 & 1 & 9 & 36 & 84 & 126 & 126 & 84 & 36 & 9 & 1 \\
\end{tabular}
\end{table}

On peut l'utiliser pour retrouver que $\parmi{4}{6}=20$.

On retrouve aussi que \[ \parmi{p}{n} = \parmi{p}{n-1} + \parmi{p-1}{n-1} \].

\subsection{Applications}

\paragraph{Application}
Comme une fonction, mais tous les éléments de l'ensemble de départ doivent absolument avoir une image (on ne peut pas avoir de valeurs interdites).

\paragraph{Application surjective}
Tout élément de l'ensemble d'arrivée a au moins un antécédent.

\section{Variables}

\subsection{Événements \& probabilités simples}

\paragraph{Un événement élémentaire} est un élément qui ne peut pas être séparé entre d'autres éléments (eg. dans un lancer de dés, ``tirer 1'' est un événement élémentaire).

\paragraph{Un événement non-élémentaire} peut être défini comme un ensemble d'événements élémentaires (eg. dans un lancer de dés, ``tirer un nombre pair'' est un événement non-élémentaire).

\paragraph{L'univers} est l'ensemble de tous les événements élémentaires, on le note $\Omega$.

\paragraph{Le cardinal} est le nombre d'événements élémentaires dans un événement non-élémentaire (la taille), on le note $card(X)$ où $X$ est l'ensemble.

\paragraph{Calcul de probabilité} La probabilité d'un événement $E$ est notée $P(E)$ et vaut \[ P(E) = \frac{card(E)}{card(\Omega} \]

\paragraph{Probabilité conditionnelle}
``La probabilité de A quand B est vérifiée'' s'écrit: \[ P_B(A)=\frac{P(A \cap B)}{P(B)} \]

\subsection{Variables aléatoires}

L'espérance est la moyenne des probabilités : \[ E(X) = \sum x_i P(X=x_i) \]

Pour deux variables aléatoires $X$ et $Y$, $E(X+Y) = E(X) + E(Y)$.

Par contre, $E(X \times Y) = E(X) \times E(Y)$ uniquement si les deux événements sont indépendants.

\paragraph{Exemple}

On jette un dé non truqué, on note la variable $X$ qui vaut le double de sa valeur. L'univers est $\Omega=\lbrace 2, 4, 6, 8, 10, 12 \rbrace$. Voir le \autoref{tab:var-alea-1}.

\begin{table}[h]
\caption{\label{tab:var-alea-1} Exemple de variable aléatoire: double du nombre tiré par un dé}
\centering
\begin{tabular}{c|cccccc}
$X=x_i$ & 2 & 4 & 6 & 8 & 10 & 12\\
\hline
$P(X=x_i)$ & $1/6$ & $1/6$ & $1/6$ & $1/6$ & $1/6$ & $1/6$\\
\end{tabular}
\end{table}

On pose une variable $Y$ qui vaut 1 si le résultat est pair et 3 si le résultat est impair. Puisque les chances sont équiprobables, $P(Y=1) = \frac{1}{2}$ et $P(Y=3) = \frac{1}{2}$.

On s'intéresse à $X+Y$ et $X \times Y$. Le cas où $Y=3$ et $X=2$ est impossible, parce que cela veut dire que le nombre d'origine du lancer de dé était 1 et pair. On mets 0 à chacune de ces possibilités; voir \autoref{tab:var-alea-2} et \autoref{tab:var-alea-3}.

\begin{table}[h]
\caption{\label{tab:var-alea-2} Somme de deux variables aléatoires}
\centering
\begin{tabular}{c|cc}
$X \backslash Y$ & 1 & 3\\
\hline
2 & 3 & 0\\
4 & 0 & 7\\
6 & 7 & 0\\
8 & 0 & 11\\
10 & 11 & 0\\
12 & 0 & 15\\
\end{tabular}
\end{table}

\begin{table}[h]
\caption{\label{tab:var-alea-3} Probabilités de $P(X+Y = x_i)$}
\centering
\begin{tabular}{c|cccc}
$x_i$ & 3 & 7 & 11 & 15\\
\hline
$P(X+Y=x_i)$ & 1/6 & 2/6 & 2/6 & 1/6\\
\end{tabular}
\end{table}

\subsection{Variables discrètes}

On note $E(X)$ l'espérance (la moyenne).
\[
E(X) = \sum_{i=1}^n x_i P(x_i)
\]

La fonction de répartition, $F(k)$, est la probabilité que plusieurs événements se réalisent.
\[
F(k) = P(X \leq k) = \sum_{i=1}^k P(x_i)
\]

La variance est notée $V(X)$ ;
\[
V(X) = E[(X-E(X))^2] = E(X^2) - E(X)^2
\]

L'écart-type est noté $\sigma(X)$ ;
\[
\sigma(X) = \sqrt{V(X)}
\]

\subsection{Variables continues}

L'espérance $E(X)$ est:
\[
E(X)= \int_a^x f(t) \text{d} t
\]

La fonction de répartition $F(u)$ est:
\[
F(u) = \int_a^u f(t) \text{d} t
\]

\section{Lois discrètes}

\subsection{Loi binomiale}

Dans le cas d'une suite d'expériences élémentaires indépendantes à deux issues possibles, on utilise la loi binomiale.

On note $p$ la probabilité d'une réussite et $n$ le nombre d'expériences (par exemple pour 12 lancers de pièce: $p=1/2$ et $n=12$). On note cette loi $B(n; p)$, dite ``de Bernouilli''. On peut alors créer la variable aléatoire suivant cette loi : \[ X \suit B(n; p) \]

On peut calculer la probabilité qu'il y ai $k$ succès (où $k \in [0; n]$):
\[
P(X=k) = \parmi{k}{n} p^k (1-p)^{n-k}
\]

\paragraph{Exemple}
Pour un lancer de 4 fléchettes successivement avec une probabilité de réussite de $0.6$, on a $p=0.6$ et $n=4$ donc la probabilité de toucher exactement 3 fois est \[ P(X=3) = \parmi{3}{4} \times 0.6^3 \times 0.4^1 \].

L'espérance $E(X)$ s'écrit: \[E(X)=n \times p\]

L'écart-type $\sigma(X)$ s'écrit: \[\sigma(X)=\sqrt{np(1-p)}\]

La variance $V(X)$ s'écrit: \[V(X) = \sigma(X)^2\]

\subsection{Loi poisson}
Dans le cas où il s'agit d'un événement rare (la probabilité $p$ est faible et le nombre d'essais $n$ est élevé), on peut approximer la loi binomiale par la loi poisson pour obtenir la probabilité de réussite dans un intervalle de temps (eg. apparition de pannes dans un réseau informatique). La loi de poisson correspond à la loi binomiale $B(n, \frac{\lambda}{n})$. On peut donc trouver :
\[
P(X = k) \approx \lim_{n \to \infinity} P(X_n = k) = \exp(-\lambda) \frac{\lambda^k}{k!}
\] où $\lambda$ est le nombre de fois que l'événement se produit en moyenne ($E(X) = \lambda$).

On peut chercher soit ``le nombre de réalisations dans un intervalle donné'' ou ``le temps entre deux réalisations d'un événement''.

Deux événements successifs doivent être indépendants.

L'événement doit être suffisamment rare pour ne pas se réaliser plusieurs fois en un petit intervalle de temps.

Quand il y a plus de 20 tentatives ($n$), avec une probabilité plus petite que 0.1 ($p$), et il faut que la moyenne ($np$) soit plus petit que 5. Dans ce cas : \[B(n; p) \approx P(np)\]

\subsection{Loi géométrique}

La loi géométrique est une variante de la loi binomiale : au lieu de chercher le ``nombre de réussites en $n$ essais'', on cherche ``combien d'essais sont nécessaires pour obtenir une réussite''.
\[
P(X = n) = q^{n-1} p
\] où $p$ est la probabilité, $q$ la probabilité de l'échec (donc $q = 1-p$) et $P(X = n)$ est la probabilité de réussir au $n$\ieme~tirage.

On peut couper la formule entre $q^{n-1}$ qui est la probabilité d'avoir $n-1$ échecs à la suite, puis $p$ la probabilité d'avoir un succès (``avoir un succès au bout du $n$\ieme~tirage'' = ``avoir $n-1$ échecs'' puis ``réussir une fois'').

On trouve l'espérance: \[E(X) = \frac{1}{q}\]

La variance: \[V(X) = \frac{q}{p^2}\]

L'écart-type: \[\sigma(X) = \frac{\sqrt{q}}{p}\]

La loi géométrique est une loi sans mémoire : on peut ignorer le passé : donc on peut ignorer les probabilités conditionnelles.

\section{Lois continues}

\subsection{Loi normale}

La loi normale a deux paramètres: \[N(E, \sigma)\].

On peut donc approximer une loi binomiale par une loi normale: \[ B(n, p) \approx N(np, \sqrt{npq}) \] dès que $n \geq 30$, $np \geq 15$ et $nq \geq 15$.

On peut aussi approximer une loi poisson par une loi normale: \[P(\lambda) \approx N(\lambda, \sqrt{\lambda})\] dès que $\lambda \geq 16$.

Cela permet de trouver des cas comme $P(X \leq k)$ où $k$ est un entier.

\paragraph{Remarque}
Pour $k$ un entier, $P(X = k) = 0$.

\paragraph{Correction de continuité}
Puisque l'on passe d'une loi discrète à une loi continue, il faut transformer les valeurs: \[ P(X > n) = P(X \geq n+1) \approx P(X \geq n+0.5) \]
par exemple, $P(X > 300)$ devient $P(X \geq 300.5)$.

\paragraph{Calcul de probabilités grâce à une table}

La table permet de trouver la probabilité de $P(X < n)$ où $n \geq 0$ ; pour $X \suit N(0, 1)$.

Si on veut calculer d'autres probabilités, il faut les remplacer:
\begin{itemize}
\item $P(X > n) = 1 - P(X < n)$
\item $P(X < -n) = 1 - P(X < n)$
\item $P(X > -n) = P(X < n)$
\item $P(a < X < b) = P(X < b) - P(X < a)$ où $(a,b) \in \setR^2$
\item $P(\vert X \vert > n) = P(Z > n \text{ ou } Z < -n) = 2 P(Z > n) = 2 (1-P(Z < n))$
\end{itemize}

\paragraph{Centrer une loi}
Pour une variable aléatoire $X$ suivant une loi normale de paramètres $\mu, \sigma$; on peut la centrer en: 
\[Y = \frac{X - \mu}{\sigma} \suit N(0, 1)\]

\subsection{Loi exponentielle}

La loi exponentielle est définie par:
\begin{itemize}
\item Si $x < 0$ : 0
\item Si $x \geq 0$ : $\lambda e^{-\lambda x}$
\end{itemize}

On l'utilise très souvent pour des durées de vie.

L'espérance est \[E(X) = \frac{1}{\lambda}\]

L'écart-type est \[ \sigma(X) = \frac{1}{\lambda} \]

Pour calculer $P(a < X < b)$ : \[ \int_a^b \lambda e^{-\lambda t} dt = \left[ -e^{-\lambda t} \right]_a^b = -e^{-\lambda b} + e^{-\lambda a} \]

Par exemple pour $P(50 < X < 100)$ : \[ \int_{50}^{100} \lambda e^{-\lambda t} dt = \left[ -e^{-\lambda t} \right]_{50}^{100} = -e^{-\lambda 100} + e^{-\lambda 50} \]

Si on cherche $P(X \geq n)$, on calcule l'inverse: \[ P(X \geq n) = 1 - P(X \leq n) \] ce qui est beaucoup plus simple à calculer.

\paragraph{Remarque}
La loi exponentielle est une loi sans vieillissement: \[ P_B(a < X < b) = P(a < X < b) \]

\section{Comment choisir la loi}

La loi d'origine est soit:

\begin{itemize}
\item Une \textbf{loi binomiale} dans la plupart des cas, de paramètres $p$ la probabilité et $n$ le nombre d'essais,
\item Une \textbf{loi poisson} dans les cas où les événements sont rares, de paramètre $\lambda$ la moyenne de réussites dans une période donnée, quand on cherche le temps nécessaire,
\item Une \textbf{loi géométrique} si on cherche le nombre d'essais nécessaires pour le premier échec (ou la première réussite),
\item S'il s'agit d'événements dépendants, on peut aussi utiliser les \textbf{probabilités conditionnelles}.
\item Si on dépend du temps, qu'il y a diminution, et qu'on a une moyenne de décroissance, on utilise la \textbf{loi exponentielle}.
\item On peut convertir les trois premières lois en \textbf{loi normale} pour obtenir des probabilités ``inférieure'' de la forme $P(X \leq k)$.
\end{itemize}

\part{Statistiques}

\section{Analyse unidimensionnelle}

On s'intéresse à une seule variable aléatoire, souvent notée $X$.

\subsection{Caractère discret}

Pour une population de $n$, on a plusieurs valeurs appelées $v_0, v_1 \ldots v_n$.

Plusieurs individus peuvent avoir une même valeur (par exemple, plus élèves peuvent avoir la même note) : on note $x$ chaque valeur possibles; donc $x_0, x_1 \ldots x_k$. À noter que $k \leq n$ puisqu'on ne peut pas avoir plus de résultats que d'individus !

\paragraph{Effectif}
L'effectif de la variable $x_i$ correspond au nombre d'individus qui ont eu ce résultat (par exemple, le nombre d'étudiants ayant eu 15): on le note $n_i$.

\paragraph{Fréquence}
La fréquence de la variable $x_i$ est la ``probabilité'' que cela arrive; donc \[ f_i = \frac{n_i}{n} \]

\paragraph{Fréquence cumulée}
La fréquence cumulée est la somme de toutes les fréquences inférieures: \[ F_i = \sum_{j=1}^i f_j \]

\subsection{Paramètres de position}

\paragraph{Mode}
Pour trouver le mode, on calcule chaque fréquence. La fréquence la plus élevée permet de trouver le mode. Par exemple, si on a 70\% des étudiants qui ont eu 12, 20\% qui ont eu 13 et 10\% qui ont eu 14, le mode est 12.

\paragraph{Moyenne}
Notée $\overline{X}$ \[ \overline{X} = \frac{1}{n} \sum_{i=1}^k n_i x_i \]

\paragraph{Médiane}
La valeur telle que la moitié des valeurs est inférieure, et l'autre moitié est supérieure ; notée $M$.

\paragraph{1\ier~quartile}
La valeur telle que 25\% des valeurs sont inférieures, et 75\% des valeurs sont supérieures ; notée $Q_1$.

\paragraph{3\ieme~quartile}
La valeur telle que 75\% des valeurs sont inférieures, et 25\% des valeurs sont supérieures ; notée $Q_3$.

\subsection{Paramètres de dispersion}

\paragraph{Écart absolu moyen}
%TODO c'est un peu l'écart-type ?

\paragraph{Intervalle de variation}
L'espace entre la valeur maximale et la valeur minimale: \[ v_{max} - v_{min} \]

\paragraph{Intervalle inter-quartile}
L'espace entre les deux quartiles: \[ Q_3 - Q_1 \]

\paragraph{Variance}
%TODO

\section{Analyse bidimensionnelle}

L'analyse dimensionnelle permet d'étudier des corrélations.

\paragraph{Covariance}
\[ cov(X, Y) = \sigma_{X Y} = \frac{\sum_{i=1}^n (x_i - \overline{X}) (y_i - \overline{Y})}{n} \]
\[ \sigma_{X Y} = \overline{X Y} - \overline{X} \times \overline{Y} \]

\paragraph{Coefficient de corrélation}
\[ \rho_{X Y} = \frac{\sigma_{X Y}}{\sigma_X \sigma_Y} \]
avec
\[ -1 \leq \rho_{X Y} \leq 1 \]
on a aussi \[ \rho_{X Y} = \frac{<X', Y'>}{\Vert X \Vert \Vert Y \Vert} = cos(X', Y') \]

Si $\vert \rho_{X Y} \vert \approx 1$ veut dire que les deux séries sont corrélées.

\end{document}